<!DOCTYPE html>
<html lang="english">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Marek Kolodziej" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="ring allreduce, allreduce, MPI, NCCL, bandwith optimal, communication collective, communication, collective, HPC, " />

<meta property="og:title" content="Allreduce - the basis of multi-device communication for neural network training "/>
<meta property="og:url" content="/allreduce-the-basis-of-multi-device-communication-for-neural-network-training.html" />
<meta property="og:description" content="Introduction For big deep learning models and datasets, training on a single GPU (or TPU or CPU) may not be fast enough. For example, according to NVIDIA&#39;s Megatron-LM code base, training BERT Large takes 3 days on 64 Tesla V100 GPUs. Assuming linear scaling, this would mean that training on …" />
<meta property="og:site_name" content="Marek Kolodziej&#39;s Blog" />
<meta property="og:article:author" content="Marek Kolodziej" />
<meta property="og:article:published_time" content="2019-08-15T16:40:00-07:00" />
<meta name="twitter:title" content="Allreduce - the basis of multi-device communication for neural network training ">
<meta name="twitter:description" content="Introduction For big deep learning models and datasets, training on a single GPU (or TPU or CPU) may not be fast enough. For example, according to NVIDIA&#39;s Megatron-LM code base, training BERT Large takes 3 days on 64 Tesla V100 GPUs. Assuming linear scaling, this would mean that training on …">

        <title>Allreduce - the basis of multi-device communication for neural network training  · Marek Kolodziej&#39;s Blog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/admonition.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="/"><span class=site-name>Marek Kolodziej's Blog</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       "/"
                                    >Home</a>
                                </li>
                                <li ><a href="/categories">Categories</a></li>
                                <li ><a href="/tags">Tags</a></li>
                                <li ><a href="/archives">Archives</a></li>
                                <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="/allreduce-the-basis-of-multi-device-communication-for-neural-network-training.html">
                Allreduce - the basis of multi-device communication for neural network training
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h3>Introduction</h3>
<p>For big deep learning models and datasets, training on a single GPU (or TPU or CPU) may not be fast enough. For example, according to NVIDIA's <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> code base, training BERT Large takes 3 days on 64 Tesla V100 GPUs. Assuming linear scaling, this would mean that training on a single GPU would have taken 192 days, or more than 6 months! Of course, scaling is hardly ever perfectly linear, but even if we assume 50% efficiency, that would have still meant 3 months instead of 3 days. Scaling is what makes certain problems feasible that used not to be viable with smaller compute resources. Big O of a task may not change, but scaling up the processing of a problem with a given complexity can make a difference. </p>
<p>The benefit of mini-batch stochastic gradient descent (<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>) and related optimization algorithms is that they take a few examples a time and run the forward and backward passes with enough data at once to keep the hardware busy. For example, in case of the GPU, we want at least have enough data to exploit <a href="https://en.wikipedia.org/wiki/Data_parallelism">data parallelism</a> for <a href="https://softwareengineering.stackexchange.com/questions/140534/what-makes-an-application-memory-bandwidth-bound">bandwidth-bound</a> layers, and to get both data parallelism and <a href="http://wgropp.cs.illinois.edu/courses/cs598-s16/lectures/lecture11.pdf">data reuse</a> for <a href="https://en.wikipedia.org/wiki/CPU-bound">compute-bound</a> operations such as matrix multiplication or convolution. The same logic applies to multi-GPU and multi-node processing - given a per device batch size N and K devices, we could simply sample a larger batch KN and still have N examples to work with per device. Typically, as the batch size grows, we can increase the learning rate and take fewer steps over the data, with each step contributing to a larger update of the weights. This actually doesn't always work well without various heuristics (e.g. <a href="https://arxiv.org/pdf/1706.02677.pdf">1</a>, <a href="https://arxiv.org/pdf/1711.04325.pdf">2</a>, <a href="https://arxiv.org/pdf/1708.03888.pdf">3</a>), but with the heuristics, we can scale training and converge much faster on many GPUs/TPUs/CPUs or many multi-device nodes. </p>
<p>Training on multiple devices requires communication between devices, so let's now talk about communication points. </p>
<h3>Deep learning training communication patterns</h3>
<p>The forward pass is simple - it's an <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a> problem, meaning that each device does its own work and there's no need for communication. The only thing that might need to be communicated in theory is the random seed, since different devices or nodes need to read different chunks of data to pick up different examples for their mini-batch. This could be done right at the beginning, by broadcasting the seed to each device from a master node, but let's ignore that for now since broadcast is easy. Ignoring the random seed or batch subdivision component, the forward pass doesn't need any communication.</p>
<p>The backward pass is not so simple - we calculate gradients based on a different sub-batch on each device, but then we want to average the gradients from different devices to get the overall gradient estimste. Once we get that estimate, we'd like to apply the same averaged gradient to the weight copies on each device, which are kept there for data locality. This will ensure that the weight copies remain synchronized after each iteration. Clearly, we'll need inter-device (or inter-node) communication for the gradient averaging.</p>
<p>Gradient averaging is achieved via an operation called allreduce. Let's assume that we have 4 devices, and 1 weight that we need to synchronize. All we need to do is to sum up all the values and divide by the number of devices to get the mean, then communicate the mean to all the devices.</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="kp">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.46947439</span><span class="p">,</span>  <span class="mf">0.54256004</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.46341769</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.46572975</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="kp">mean</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="kp">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.21401545</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21401545</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21401545</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21401545</span><span class="p">]])</span>
</pre></div>


<p>Of course, this looks simple, but how would we do it efficiently? That deepends on the network topology. We could have various topologies, such as a <a href="https://en.wikipedia.org/wiki/Fat_tree">fat tree</a>, <a href="https://en.wikipedia.org/wiki/Mesh_networking">mesh</a>, <a href="https://en.wikipedia.org/wiki/Ring_network">ring</a>, etc. Here, we will focus on a ring topology, because it's a popular setup, and one that can very efficiently utilize network bandwidth.</p>
<h3>Communication on a ring</h3>
<h1>TODO - work in progress</h1>
<h3>Ring Allreduce - a bandwidth-optimal algorithm</h3>
<h3>Reduce-Scatter</h3>
<h3>AllGather</h3>
<h1>TODO - work in progress</h1>
<h3>Simple Python code</h3>
<p>Here's a code sample for ring allreduce. Insteead of setting up full multi-threaded communication, let's just demonstrate it in Python, to show how things work algorithmically. Later on, we can do the the same thing with many threads and say C++. The reason it's good to start with a single-threaded approach here is because we need to make sure we get the indexing right first. The concurrent version will need to worry about other things, such as not sending a message for step i if it already updated its value for step i+1. Synchronization will be important of course, but even getting indexing right takes some effort, so let's start there.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">reduce_scatter</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">REDUCE-SCATTER</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">gpus</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gpus</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;step {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gpus</span><span class="p">):</span>
            <span class="n">idx_to_send</span> <span class="o">=</span> <span class="p">(</span><span class="n">gpu</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span>
            <span class="n">prev_idx_to_send</span> <span class="o">=</span> <span class="p">(</span><span class="n">gpu</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span>
            <span class="n">idx_to_recv</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_to_send</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span>                
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;gpu {} sends chunk {}, receives chunk {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="n">idx_to_send</span><span class="p">,</span> <span class="n">idx_to_recv</span><span class="p">))</span>
            <span class="n">chunks</span><span class="p">[</span><span class="n">gpu</span><span class="p">][</span><span class="n">idx_to_recv</span><span class="p">]</span> <span class="o">+=</span> <span class="n">chunks</span><span class="p">[(</span><span class="n">gpu</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span><span class="p">][</span><span class="n">prev_idx_to_send</span><span class="p">]</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Current state:&quot;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">chunks</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chunks</span>

<span class="k">def</span> <span class="nf">allgather</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ALLGATHER</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">gpus</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gpus</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;step {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gpus</span><span class="p">):</span>
            <span class="n">idx_to_send</span> <span class="o">=</span> <span class="p">(</span><span class="n">gpu</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span>
            <span class="n">prev_idx_to_send</span> <span class="o">=</span> <span class="p">(</span><span class="n">gpu</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span>
            <span class="n">idx_to_recv</span> <span class="o">=</span> <span class="p">(</span><span class="n">gpu</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span>
            <span class="n">chunks</span><span class="p">[</span><span class="n">gpu</span><span class="p">][</span><span class="n">idx_to_recv</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[(</span><span class="n">gpu</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">gpus</span><span class="p">][</span><span class="n">prev_idx_to_send</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;gpu {} sends chunk {}, receives chunk {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="n">idx_to_send</span><span class="p">,</span> <span class="n">idx_to_recv</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Current state:&quot;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">chunks</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chunks</span>
</pre></div>


<p>Let's now run it with the logging to see what's going on:</p>
<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s1">&#39;a0&#39;</span><span class="p">,</span> <span class="s1">&#39;b0&#39;</span><span class="p">,</span> <span class="s1">&#39;c0&#39;</span><span class="p">,</span> <span class="s1">&#39;d0&#39;</span><span class="p">,</span> <span class="s1">&#39;e0&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;a1&#39;</span><span class="p">,</span> <span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="s1">&#39;c1&#39;</span><span class="p">,</span> <span class="s1">&#39;d1&#39;</span><span class="p">,</span> <span class="s1">&#39;e1&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;a2&#39;</span><span class="p">,</span> <span class="s1">&#39;b2&#39;</span><span class="p">,</span> <span class="s1">&#39;c2&#39;</span><span class="p">,</span> <span class="s1">&#39;d2&#39;</span><span class="p">,</span> <span class="s1">&#39;e2&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;a3&#39;</span><span class="p">,</span> <span class="s1">&#39;b3&#39;</span><span class="p">,</span> <span class="s1">&#39;c3&#39;</span><span class="p">,</span> <span class="s1">&#39;d3&#39;</span><span class="p">,</span> <span class="s1">&#39;e3&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;a4&#39;</span><span class="p">,</span> <span class="s1">&#39;b4&#39;</span><span class="p">,</span> <span class="s1">&#39;c4&#39;</span><span class="p">,</span> <span class="s1">&#39;d4&#39;</span><span class="p">,</span> <span class="s1">&#39;e4&#39;</span><span class="p">]</span>
<span class="p">]</span>

<span class="n">allgather</span><span class="p">(</span><span class="n">reduce_scatter</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</pre></div>


<p>The output is as follows:</p>
<div class="highlight"><pre><span></span>REDUCE-SCATTER

step 0
gpu 0 sends chunk 0, receives chunk 4
gpu 1 sends chunk 1, receives chunk 0
gpu 2 sends chunk 2, receives chunk 1
gpu 3 sends chunk 3, receives chunk 2
gpu 4 sends chunk 4, receives chunk 3

Current state:
[[&#39;a0&#39; &#39;b0&#39; &#39;c0&#39; &#39;d0&#39; &#39;e0e4&#39;]
 [&#39;a1a0&#39; &#39;b1&#39; &#39;c1&#39; &#39;d1&#39; &#39;e1&#39;]
 [&#39;a2&#39; &#39;b2b1&#39; &#39;c2&#39; &#39;d2&#39; &#39;e2&#39;]
 [&#39;a3&#39; &#39;b3&#39; &#39;c3c2&#39; &#39;d3&#39; &#39;e3&#39;]
 [&#39;a4&#39; &#39;b4&#39; &#39;c4&#39; &#39;d4d3&#39; &#39;e4&#39;]] 

step 1
gpu 0 sends chunk 4, receives chunk 3
gpu 1 sends chunk 0, receives chunk 4
gpu 2 sends chunk 1, receives chunk 0
gpu 3 sends chunk 2, receives chunk 1
gpu 4 sends chunk 3, receives chunk 2

Current state:
[[&#39;a0&#39; &#39;b0&#39; &#39;c0&#39; &#39;d0d4d3&#39; &#39;e0e4&#39;]
 [&#39;a1a0&#39; &#39;b1&#39; &#39;c1&#39; &#39;d1&#39; &#39;e1e0e4&#39;]
 [&#39;a2a1a0&#39; &#39;b2b1&#39; &#39;c2&#39; &#39;d2&#39; &#39;e2&#39;]
 [&#39;a3&#39; &#39;b3b2b1&#39; &#39;c3c2&#39; &#39;d3&#39; &#39;e3&#39;]
 [&#39;a4&#39; &#39;b4&#39; &#39;c4c3c2&#39; &#39;d4d3&#39; &#39;e4&#39;]] 

step 2
gpu 0 sends chunk 3, receives chunk 2
gpu 1 sends chunk 4, receives chunk 3
gpu 2 sends chunk 0, receives chunk 4
gpu 3 sends chunk 1, receives chunk 0
gpu 4 sends chunk 2, receives chunk 1

Current state:
[[&#39;a0&#39; &#39;b0&#39; &#39;c0c4c3c2&#39; &#39;d0d4d3&#39; &#39;e0e4&#39;]
 [&#39;a1a0&#39; &#39;b1&#39; &#39;c1&#39; &#39;d1d0d4d3&#39; &#39;e1e0e4&#39;]
 [&#39;a2a1a0&#39; &#39;b2b1&#39; &#39;c2&#39; &#39;d2&#39; &#39;e2e1e0e4&#39;]
 [&#39;a3a2a1a0&#39; &#39;b3b2b1&#39; &#39;c3c2&#39; &#39;d3&#39; &#39;e3&#39;]
 [&#39;a4&#39; &#39;b4b3b2b1&#39; &#39;c4c3c2&#39; &#39;d4d3&#39; &#39;e4&#39;]] 

step 3
gpu 0 sends chunk 2, receives chunk 1
gpu 1 sends chunk 3, receives chunk 2
gpu 2 sends chunk 4, receives chunk 3
gpu 3 sends chunk 0, receives chunk 4
gpu 4 sends chunk 1, receives chunk 0

Current state:
[[&#39;a0&#39; &#39;b0b4b3b2b1&#39; &#39;c0c4c3c2&#39; &#39;d0d4d3&#39; &#39;e0e4&#39;]
 [&#39;a1a0&#39; &#39;b1&#39; &#39;c1c0c4c3c2&#39; &#39;d1d0d4d3&#39; &#39;e1e0e4&#39;]
 [&#39;a2a1a0&#39; &#39;b2b1&#39; &#39;c2&#39; &#39;d2d1d0d4d3&#39; &#39;e2e1e0e4&#39;]
 [&#39;a3a2a1a0&#39; &#39;b3b2b1&#39; &#39;c3c2&#39; &#39;d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b4b3b2b1&#39; &#39;c4c3c2&#39; &#39;d4d3&#39; &#39;e4&#39;]] 


ALLGATHER

step 0
gpu 0 sends chunk 1, receives chunk 0
gpu 1 sends chunk 2, receives chunk 1
gpu 2 sends chunk 3, receives chunk 2
gpu 3 sends chunk 4, receives chunk 3
gpu 4 sends chunk 0, receives chunk 4

Current state:
[[&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c0c4c3c2&#39; &#39;d0d4d3&#39; &#39;e0e4&#39;]
 [&#39;a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d1d0d4d3&#39; &#39;e1e0e4&#39;]
 [&#39;a2a1a0&#39; &#39;b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e2e1e0e4&#39;]
 [&#39;a3a2a1a0&#39; &#39;b3b2b1&#39; &#39;c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b4b3b2b1&#39; &#39;c4c3c2&#39; &#39;d4d3&#39; &#39;e3e2e1e0e4&#39;]] 

step 1
gpu 0 sends chunk 0, receives chunk 4
gpu 1 sends chunk 1, receives chunk 0
gpu 2 sends chunk 2, receives chunk 1
gpu 3 sends chunk 3, receives chunk 2
gpu 4 sends chunk 4, receives chunk 3

Current state:
[[&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c0c4c3c2&#39; &#39;d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d1d0d4d3&#39; &#39;e1e0e4&#39;]
 [&#39;a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e2e1e0e4&#39;]
 [&#39;a3a2a1a0&#39; &#39;b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b4b3b2b1&#39; &#39;c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]] 

step 2
gpu 0 sends chunk 4, receives chunk 3
gpu 1 sends chunk 0, receives chunk 4
gpu 2 sends chunk 1, receives chunk 0
gpu 3 sends chunk 2, receives chunk 1
gpu 4 sends chunk 3, receives chunk 2

Current state:
[[&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e2e1e0e4&#39;]
 [&#39;a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]] 

step 3
gpu 0 sends chunk 3, receives chunk 2
gpu 1 sends chunk 4, receives chunk 3
gpu 2 sends chunk 0, receives chunk 4
gpu 3 sends chunk 1, receives chunk 0
gpu 4 sends chunk 2, receives chunk 1

Current state:
[[&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]
 [&#39;a4a3a2a1a0&#39; &#39;b0b4b3b2b1&#39; &#39;c1c0c4c3c2&#39; &#39;d2d1d0d4d3&#39; &#39;e3e2e1e0e4&#39;]] 
</pre></div>


<h1>TODO - work in progress</h1>
<h3>Real concurrent code in C++</h3>
<h1>TODO: work in progress</h1>


            
            
            
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2019-08-15T16:40:00-07:00">Aug 15, 2019</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#hpc-ref">HPC</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags#allreduce-ref">allreduce
                    <span>1</span>
</a></li>
                <li><a href="/tags#bandwith-optimal-ref">bandwith optimal
                    <span>1</span>
</a></li>
                <li><a href="/tags#collective-ref">collective
                    <span>1</span>
</a></li>
                <li><a href="/tags#communication-ref">communication
                    <span>1</span>
</a></li>
                <li><a href="/tags#communication-collective-ref">communication collective
                    <span>1</span>
</a></li>
                <li><a href="/tags#mpi-ref">MPI
                    <span>1</span>
</a></li>
                <li><a href="/tags#nccl-ref">NCCL
                    <span>1</span>
</a></li>
                <li><a href="/tags#ring-allreduce-ref">ring allreduce
                    <span>1</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="#" title="My You can add links in your config file Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-you can add links in your config file sidebar-social-links"></i></a>
    <a href="#" title="My Another social link Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-another social link sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>