<!DOCTYPE html>
<html lang="english">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Marek Kolodziej" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="matmul, GEMM, linear algebra, " />

<meta property="og:title" content="Matrix Multiplication on CPU, GPU and TPU "/>
<meta property="og:url" content="/matrix-multiplication-on-cpu-gpu-and-tpu.html" />
<meta property="og:description" content="Hidden GEMMs: How to optimize matrix multiplication on CPU and GPU TODO: TPU Importance of matrix multiplication Matrix multiplication is a fundamental operation in scientific computing. Here are just a few of countless uses of this fundamental linear algebra operation: compactly notating systems of linear equations solving least squares problems …" />
<meta property="og:site_name" content="Marek Kolodziej&#39;s Blog" />
<meta property="og:article:author" content="Marek Kolodziej" />
<meta property="og:article:published_time" content="2019-08-05T16:53:00-07:00" />
<meta name="twitter:title" content="Matrix Multiplication on CPU, GPU and TPU ">
<meta name="twitter:description" content="Hidden GEMMs: How to optimize matrix multiplication on CPU and GPU TODO: TPU Importance of matrix multiplication Matrix multiplication is a fundamental operation in scientific computing. Here are just a few of countless uses of this fundamental linear algebra operation: compactly notating systems of linear equations solving least squares problems …">

        <title>Matrix Multiplication on CPU, GPU and TPU  · Marek Kolodziej&#39;s Blog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/admonition.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="/"><span class=site-name>Marek Kolodziej's Blog</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       "/"
                                    >Home</a>
                                </li>
                                <li ><a href="/categories">Categories</a></li>
                                <li ><a href="/tags">Tags</a></li>
                                <li ><a href="/archives">Archives</a></li>
                                <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="/matrix-multiplication-on-cpu-gpu-and-tpu.html">
                Matrix Multiplication on CPU, GPU and TPU
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h2>Hidden GEMMs: How to optimize matrix multiplication on CPU and GPU</h2>
<h3>TODO: TPU</h3>
<h3>Importance of matrix multiplication</h3>
<p>Matrix multiplication is a fundamental operation in scientific computing. Here are just a few of countless uses of this fundamental linear algebra operation:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Matrix_multiplication#System_of_linear_equations">compactly notating</a> systems of linear equations</li>
<li>solving least squares problems, e.g. linear regression (both the <a href="https://towardsdatascience.com/analytical-solution-of-linear-regression-a0e870b038d5">analytical</a> and the <a href="https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843">iterative</a> solution)</li>
<li>characteristic equations in differential equations are based on eigenvalue decomposition</li>
<li>dimensionality reduction (<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>)</li>
<li>decompositions such as <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a> (since <span class="math">\(M=U \Sigma V^*\)</span>), which can be used e.g. in topic modeling (<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">LSA</a>)</li>
<li>developing search result ranking algorithms (e.g. <a href="https://en.wikipedia.org/wiki/PageRank#Python">PageRank</a>)</li>
<li>finding the <a href="https://en.wikipedia.org/wiki/Transitive_closure">transitive closure</a> in graphs</li>
<li>solving the <a href="http://www.cs.tau.ac.il/~zwick/Adv-Alg-2015/Matrix-Graph-Algorithms.pdf">all-pairs shortest path</a> (APSP) problem</li>
</ul>
<p>Since this blog is about machine learning and mostly deep learning, the primary application of interest to the reader will most likely be the implementation of <a href="https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528">fully-connected layers</a> in neural networks. At its core, the fully-connected is based on matrix multiplication.</p>
<p>Convolutions can also be cast as matrix multiplications. For example, a classic approach for this is to preprocess the input using <a href="https://www.mathworks.com/help/images/ref/im2col.html">im2col</a>, do a matrix multiplication, and then apply <a href="https://www.mathworks.com/help/images/ref/col2im.html">col2im</a>. This is very inefficient, because the im2col matrix needs to be generated, and takes up a lot of extra memory. However, this approach can be made efficient using implicit im2col, resulting in an implicit GEMM convolution, which is implemented via some of the cuDNN algorithms. Also, a <span class="math">\(1x1\)</span> convolution <a href="https://datascience.stackexchange.com/questions/12830/how-are-1x1-convolutions-the-same-as-a-fully-connected-layer">can be cast</a> as a matrix multiplication even without im2col.</p>
<h3>Review of the mathematical operation</h3>
<p>Before we focus on the code, let's review the basics. If we take a matrix <span class="math">\(A_{MxK}\)</span> with <span class="math">\(M\)</span> rows and <span class="math">\(K\)</span> columns, we can multiply it by matrix <span class="math">\(B_{KxN}\)</span> with <span class="math">\(K\)</span> rows and <span class="math">\(N\)</span> columns, obtaining matrix <span class="math">\(C_{MxN}\)</span> with <span class="math">\(M\)</span> rows and <span class="math">\(N\)</span> columns. To obtain each value <span class="math">\(C_{ij}\)</span>, we calculate a dot product between the <span class="math">\(i\)</span>th row of <span class="math">\(A\)</span> and the <span class="math">\(j\)</span>th column of <span class="math">\(B\)</span>.</p>
<div class="math">$$
A = \begin{bmatrix}
    a_{11} &amp; a_{12} &amp; a_{13} \\
    a_{21} &amp; a_{22} &amp; a_{23}
  \end{bmatrix}
$$</div>
<div class="math">$$
B = \begin{bmatrix}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22} \\
b_{31} &amp; b_{32}
\end{bmatrix}
$$</div>
<div class="math">$$C = AB = \begin{bmatrix}
a_{11}*b_{11} + a_{12}*b_{21} + a_{13}*b_{31} &amp; a_{11}*b_{12} + a_{12}*b_{22} + a_{13}*b_{32} \\
a_{21}*b_{11} + a_{22}*b_{21} + a_{23}*b_{31} &amp; a_{21}*b_{12} + a_{22}*b_{22} + a_{23}*b_{32}
\end{bmatrix}$$</div>
<h3>Machine Config</h3>
<p>For any benchmark, we need to know what hardware and software configuration we're dealing with. The performance reported below was based on the following hardware and software:</p>
<ul>
<li>CPU: Intel&reg; Core&trade; i7-7800X @ 3.50 GHz</li>
<li>RAM: 32 GB DDR4 @ 2.4 GHz (G-Skill Ripjaws F4-2400C15-16GVR)</li>
<li>Motherboard: ASUS PRIME X299-A motherboard</li>
<li>OS: Ubuntu 16.04.6 LTS (Xenial Xerus)</li>
<li>Compiler: GCC 5.4.0</li>
<li>Compiler flags: <code>-O3 -march=native</code> (unless specified otherwise)</li>
</ul>
<h3>Naive Iterative Algorithm</h3>
<p>Let's assume that M = N = K = 1024 for simplicity.</p>
<p>Let's try a simple 3-loop iterative algorithm in C++:</p>
<div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">naiveIterativeMatmul</span><span class="p">(</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">A</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">B</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">C</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span>  <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">n</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>Can we see any problems with the code above? There are plenty. For one, this may be incorrect unless we ensured to <code>memset</code> matrix C to zero. Even assuming that we did so, this code runs extremely slowly. With the good compiler flags (<code>-O2 -march=native</code>, see next section), this implementation took 1,600 ms. With lower optimization levels, performance was even worse. For example, just using <code>-O2</code> without <code>-march=native</code>, I got 1,750 ms; <code>-O1</code> was 2,438 ms; and no flags took 4,501 ms.  </p>
<h3>The importance of good compiler flags</h3>
<p>As you saw above, high optimization levels result in a very significant performance improvement. Surely we wouldn't want to compare algorithms if there were plenty of scope for the compiler to further optimize code. On the other hand, <code>-march=native</code> is a bit risky.</p>
<p>Compiling for a given machine may be a bad assumption - after all, we wish the code to be reasonably portable (e.g. for code to work on both Skylake and Haswell chips). In practice, it's not as bad. For instance, we can build libraries with implementations for different architectures, and load the library compiled for the closest architecture once we know which CPU is used at runtime, e.g. via the use of <a href="https://en.wikipedia.org/wiki/CPUID">CPUID</a> style instructions. This can be done is we generate several shared libraries, for example, and then <code>dlopen</code> the one that matches. For repetitive, long-running code, another option is to just-in-time compile (<a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT</a>) such code, which is the preferred approach for optimizing code since there are added benefits that can be exploited when JITting, such as known dimensions which can be provided to the compiler as literals, rather than runtime values.</p>
<p>For now, unless stated otherwise, let's assume that we're using <code>-O3 -march=native</code> to have the compiler optimize as much as possible, so we can focus on algorithmic differences.</p>
<h3>Optimizing the naive iterative algorithm</h3>
<p>Let's try to improve performance with one line of code, while also addressing the issue of not having <code>memset</code> matrix C. Note that the <code>+=</code> is a problem - we read from RAM and then update. If a matrix is big enough, we'll surely incur some cache misses. Since we only need to update one float at a time, we could instead create a single float, which the compiler would likely put in a register, and update that register instead. This way we should go from 3 reads (A, B and C) and 1 write (C) from RAM to 2 reads and 1 write. Since the register access time is insignificant compared to RAM, we'd expect about 25% of the execution time to go away.</p>
<div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">iterativeMatmulRegisterSum</span><span class="p">(</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">A</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">B</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">C</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">n</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span>  <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>This implementation took 1,055 ms to execute, which was 65% of the time it took to execute the original. We actually gained more time than we expected! This could be for many reasons, including not polluting the cache with C matrix intermediates, not having to deal with read-write sequencing of RAM (rather than just reading), etc.</p>
<p>Note though that in a row-major programming language like C/C++, having <span class="math">\(k\)</span> as the innermost loop is a problem. That's because while for matrix <span class="math">\(A\)</span>, <span class="math">\(k\)</span> represents columns, for matrix <span class="math">\(B\)</span>, <span class="math">\(k\)</span> represents rows. If these are big matrices, we're likely to incur cache misses on every read from matrix <span class="math">\(B\)</span>! What can we do about this? As we discussed, <span class="math">\(k\)</span> represents columns of matrix <span class="math">\(A\)</span>. On the other hand, <span class="math">\(n\)</span> indexes into the columns of matrix <span class="math">\(B\)</span>. Let's leave the index that has to go down rows, i.e. <span class="math">\(k\)</span>, in the middle loop, while keeping the innermost loop working on rows of one of the matrices. For instance, let's make the loop over <span class="math">\(k\)</span> the middle loop, and the loop over <span class="math">\(n\)</span> the innermost loop.</p>
<div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">iterativeMatmulLoopReorder</span><span class="p">(</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">A</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">B</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">C</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span>  <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">n</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>The above code took only 70 ms! Not surprisingly, hitting the caches matters. This technique is known as loop reordering or <a href="https://en.wikipedia.org/wiki/Loop_interchange">loop interchange</a>.</p>
<p>Let's try one more thing. Let's assume that we have the ability to just-in-time compile a function optimal for given matrix dimensions. Say for instance that this is some hidden LSTM state which we expect to use over and over for training or inference, so we can statically determnine the dimensions. Let's see what the impact of statically known dimensions is on performance. To do this, let's template the function:</p>
<div class="highlight"><pre><span></span><span class="k">template</span> <span class="o">&lt;</span><span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">iterativeMatmulLoopReorderTempl</span><span class="p">(</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">A</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">B</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">C</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span>  <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">n</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>The above code took only 45 ms to execute! I decided to compare this runtime to GNU Octave:</p>
<div class="highlight"><pre><span></span><span class="n">x</span> <span class="p">=</span> <span class="nb">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">);</span>
<span class="n">y</span> <span class="p">=</span> <span class="nb">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">);</span>
<span class="n">tic</span><span class="p">;</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">;</span> <span class="n">toc</span><span class="p">;</span>
</pre></div>


<p>Octave tookj 35 ms. This isn't bad, we're close to performance from BLAS itself! Note that I didn't yet do any heavy optimization, such as manual vectorization (e.g. using <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a> <a href="https://en.wikipedia.org/wiki/Intrinsic_function">intrinsics</a> or <a href="https://en.wikipedia.org/wiki/Inline_assembler">inline assembly</a>), tiling, etc. Of course, note that it's unreasonable to compare a generic library such as the BLAS libraries used by Octave, NumPy, etc., against a dimension-templated function, which will only work for this particular dimension. Still, even before templating, we went down from 1,600 ms to 70 ms, and to within 2x of Octave's libraries. Let's also check NumPy (from Anaconda) while we're at it:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>


<p>NumPy took 40 ms to execute the matrix multiplication.</p>
<h3>Caching issues with the naive iterative algorithm</h3>
<p>We briefly discussed how the naive algorithm had issues with caching, because of the loop order. Unfortunately, even loop reordering isn't ideal. For one, even if data is prefetched, we're not breaking up the problem to fit out cache well. A much better way to break up the processing in a way that would fit the cache is via tiling.</p>
<p>Before we get into tiling, let's review hardware caches.</p>
<p>First of all, hardware caches are supposed to be fast. These are small chunks of memory that sit on the same chip as the CPU, and use static RAM (SRAM) instead of dynamic RAM (DRAM) like the main memory that we're used to. SRAM uses flip-flops and is much faster than DRAM, which uses capacitors. Unfortunately, SRAM takes up a huge amount of chip real estate, and uses a lot of power, which is why main RAM is dynamic. </p>
<p><strong>Insert DRAM and SRAM diagrams here!!</strong></p>
<p>This is typical of any memory hierarchy in a computer, where we have a clear speed-capacity trade-off - registers are fast but tiny, L1 cache is slightly bigger but also a bit slower, DRAM is much larger but slower still, and SSDs have the highest capacity but are slower still. Here are <a href="https://www.prowesscorp.com/computer-latency-at-a-human-scale/">some statistics</a> in CPU clock cycles that show the memory access time depending on the device being accessed.</p>
<p>Of course, these numbers may vary, they are just here to give you a general idea of the orders of magnitude.</p>
<table>
<thead>
<tr>
<th>Device</th>
<th>Access Time (ns)</th>
<th>CPU cycles</th>
</tr>
</thead>
<tbody>
<tr>
<td>register</td>
<td>0.4</td>
<td>1</td>
</tr>
<tr>
<td>L1 cache</td>
<td>0.9</td>
<td>2.25</td>
</tr>
<tr>
<td>L2 cache</td>
<td>2.8</td>
<td>7</td>
</tr>
<tr>
<td>L3 cache</td>
<td>28</td>
<td>70</td>
</tr>
<tr>
<td>DRAM</td>
<td>100</td>
<td>250</td>
</tr>
<tr>
<td>NVMe SSD</td>
<td>25,000</td>
<td>62,500</td>
</tr>
<tr>
<td>SATA SSD</td>
<td>100,000</td>
<td>250,000</td>
</tr>
</tbody>
</table>
<p>Clearly caching matters! If a CPU is sitting idle waiting thousands of cycles for memory access, it's not spending those cycles doing any computation. Needless to say, optimizing cache use will be key to improving performance of our matrix multiplication algorithm.</p>
<p>Let's cover some basics of computer caches. We won't go into too much detail, so feel to review more at your own leisure <a href="https://en.wikipedia.org/wiki/Cache_(computing)">here</a>. </p>
<p>Caches aren't exactly ideal in practice. They tend to use heuristics for eviction policies, they aren't fully <a href="https://en.wikipedia.org/wiki/Cache_placement_policies">associative</a>, etc. However, it turns out that practical caches can be assumed to be ideal for practical code analysis to simplify things. An ideal cache has several characteristics:
- It is fully associative. This means that there exists a single cache set with multiple cache lines, and a given memory block can occupy any of them. If we take a cache line to be of size 1 of some units, this would mean that the cache can be viewed as a matrix with m rows and 1 column.
- The cache has an omniscient residency/eviction policy. This assumes a perfect policy not just accounting for the past, but also the future, anticipating what may be needed next.</p>
<p>It turns out that while such assumptions are unreasonable, they can approximate a practical cache relatively well. The least-recently used assumption can replace the assumption of omniscience. Sleator and Tarjan (1985) <a href="https://www.cs.cmu.edu/~sleator/papers/amortized-efficiency.pdf">demonstrated</a> that given an omniscient cache with M bytes and Q cache misses, it can be replaced with a cache with an LRU cache with 2M bytes and 2Q cache misses. Therefore, the omniscience assumption can be approximated by a real, practical cache up to a small, known constant factor. The full associativity assumption is also true up to a point, e.g. L1 caches in recent Intel Core i7 CPUs are 8-way set associative, etc.</p>
<p>We also need to make a "tall cache" assumption, which means that the number of bytes per cache line is much less than the number of cache lines. Formally, if a cache line has B bytes and the cache has M bytes total (giving us <span class="math">\(M/B\)</span> cache lines), we assume that <span class="math">\(B^2 &lt; c*M\)</span> for <span class="math">\(c \le 1\)</span>.</p>
<p>This is also a reasonable assumption for practical caches. For example, on a Core i7-8700K, there's an L1 cache of 32 KB per core, and each cache line is 64 bytes. In this case, <span class="math">\(B^2=4,096\)</span>, which is 4KB, which is much less than 32 KB.</p>
<p>Why do we need to make a tall cache assumption? Essentially, this is because we need to be able to have multiple smaller items that are able to be stored in the cache, rather than fetching one gigantic item when we store. For instance, let's imagine that we had a single cache line of 32 KB, and we wanted to read a matrix down a row. A single read that resulted in a cache miss would result in storing contiguous 32 KB of memory in the cache, but we only needed one element per row, so the other elements for that row would be polluting the cache. Ideally, we'd want the cache line to be small, since then we could store small chunks from various locations in memory. However, the cache line has to be big enough to make contiguous reads efficient, by filling up the memory bus when the read is done. We will see more of that in the case of GPUs.</p>
<p>Given all the above, let's assume either an omniscient or LRU cache, as convenient, that is fully or nearly fully associative, with a tall cache assumption. The cache has <span class="math">\(M\)</span> bytes total and <span class="math">\(B\)</span> bytes per cache line, which means that it has <span class="math">\(\frac{M}{B}\)</span> cache lines.</p>
<h3>Iterative Algorithm with tiling</h3>
<p>TODO</p>
<h3>Loop unrolling and templated implementations</h3>
<p>Templated implementations are impossible to pre-compile for all but typical dimensions, but we can just-in-time (JIT) compile them if they repeat, by profiling for hotspots.</p>
<h3>Problems with Iterative algorithm on the CPU</h3>
<p>Tiling is not cache-agnostic. This means that if we pick a tile size that's optimal for a CPU with a given cache size and then end up running the same code on a CPU with a smaller cache size (assuming the same instruction set), the performance on the CPU with the smaller cache might end up abysmal. Note that we might essentially end up with the case of the <span class="math">\(\Theta\)</span>(<span class="math">\(n^3\)</span>) cache misses. </p>
<p>Another major concern with cache size-tuned algorithms is that they make the assumption that nothing else can run on the CPU at any given time. This might be an appropriate assumption to make if we were using a microcontroller, but on any platform with a running operating system, it's unrealistic. Even if the CPU does not need to run any other processes in the background, the kernel itself needs to context switch between OS housekeeping tasks (say interrupt servicing) and running the application. The context switch between the kernel and the application can result in purging caches to make room for the task being done by the kernel. The problem is obviously worse if the kernel needs to context switch not just between itself and our matrix multiplying application, but other <a href="https://en.wikipedia.org/wiki/User_space">userspace</a> applications.</p>
<p>Cache can be occupied by other items in a multi-process setting.</p>
<h3>Divide-and-conquer algorithm</h3>
<p>TODO</p>
<h3>What we didn't cover</h3>
<p>We clearly didn't cover vector processing on the CPU in detail, using instruction sets such as <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a> on x86 and <a href="https://en.wikipedia.org/wiki/ARM_architecture#Advanced_SIMD_(NEON)">NEON</a> on ARM. These will be covered in separate blog posts.</p>
<p>There exist algorithms that do less work than <span class="math">\(\Theta(M*N*K)\)</span>, such as <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen's algorithm</a>, but it deserves its own blog post, so I won't cover it now. Also, Strassen's algorithm can have worse numerical accuracy, so that's another reason to give it the detailed treatment that it deserves in a separate post. There are also other algorithms which has even better complexity than Strassen's algorithm, such as <a href="https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm">Coppersmith-Winograd</a>, however these algorithms have gigantic constant factors, that are big enough that the better big-O performance only pays off for big enough problems that they don't fit on modern computers (that is, they are <a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic algorithms</a>).</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


            
            
            
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2019-08-05T16:53:00-07:00">Aug 5, 2019</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#linear-algebra-ref">linear algebra</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags#gemm-ref">GEMM
                    <span>1</span>
</a></li>
                <li><a href="/tags#matmul-ref">matmul
                    <span>1</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="#" title="My You can add links in your config file Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-you can add links in your config file sidebar-social-links"></i></a>
    <a href="#" title="My Another social link Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-another social link sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>